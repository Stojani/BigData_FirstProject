{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Progettazione e realizzazione di job in MapReduce, Hive e Spark\n",
    "\n",
    "\n",
    "Progetto a cura del gruppo **Data-FaSt** (Farioli Davide, Stojani Marjo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indice trattazione\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**1. Specifiche Hardware e Software**\n",
    "  \n",
    "    \n",
    "**2. Specifiche dataset**\n",
    "\n",
    "\n",
    "**3. Realizzazione Job1**\n",
    "\n",
    "    3.1 Specifiche\n",
    "    \n",
    "    3.2 MapReduce\n",
    "    \n",
    "    3.3 Hive\n",
    "    \n",
    "    3.4 Spark\n",
    "    \n",
    "    3.5 Risultati\n",
    "    \n",
    "    3.6 Grafici\n",
    "    \n",
    "    \n",
    "**4. Realizzazione Job2**\n",
    "\n",
    "    4.1 Specifiche\n",
    "    \n",
    "    4.2 MapReduce\n",
    "    \n",
    "    4.3 Hive\n",
    "    \n",
    "    4.4 Spark\n",
    "    \n",
    "    4.5 Risultati\n",
    "    \n",
    "    4.6 Grafici\n",
    "    \n",
    "\n",
    "**5. Realizzazione Job3**\n",
    "\n",
    "    5.1 Specifiche\n",
    "    \n",
    "    5.2 MapReduce\n",
    "    \n",
    "    5.3 Hive\n",
    "    \n",
    "    5.4 Spark\n",
    "    \n",
    "    5.5 Risultati\n",
    "    \n",
    "    5.6 Grafici\n",
    "    \n",
    "**6. Conclusioni**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Specifiche Hardware e Software\n",
    "\n",
    "L'esecuzione dei Job è stata svolta sia in macchina locale che su Cluster AWS\n",
    "\n",
    "I Job in MapReduce e Spark sono stati realizzati con il linguaggio di programmazione **Python**\n",
    "\n",
    "### Macchina Locale\n",
    "#### Specifiche Hardware:\n",
    "- **Processor:** Intel Core i5 4th Gen 4310U\n",
    "\n",
    "- **Memory:** 8GB\n",
    "\n",
    "- **Storage:** 256GB (SSD)\n",
    "\n",
    "#### Specifiche Software:\n",
    "- **SO:** Linux Mint 19\n",
    "- **Python:** 3.7.*\n",
    "- **Java:** 1.8\n",
    "- **Hadoop:** 3.2.1\n",
    "- **Hive:** 2.3.7\n",
    "- **Spark:** 3.0.0\n",
    "\n",
    "### Cluster AWS\n",
    "#### Specifiche Hardware:\n",
    "- **Istanza EMR:** m5.xlarge\n",
    "\n",
    "- **Nodes:** 1 Master, 2 Core\n",
    "\n",
    "- **Storage:** S3 bucket\n",
    "\n",
    "#### Specifiche Software:\n",
    "- **Python:** 3.7.*\n",
    "- **Java:** 1.8\n",
    "- **Hadoop:** 3.2.1\n",
    "- **Hive:** 3.1.2\n",
    "- **Spark:** 3.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Specifiche dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il Dataset di riferimento **Daily Historical Stock Prices** contiene l’andamento giornaliero di\n",
    "un’ampia selezione di azioni sulla borsa di New York (NYSE) e sul NASDAQ dal 1970 al 2018. \n",
    "Il dataset è formato da due file CSV: historical_stock_prices.csv e historical_stocks.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**historical_stock_prices.csv** è composto da 20973889 righe e 8 colonne con i seguenti campi:\n",
    "\n",
    "- **ticker:** simbolo univoco dell’azione\n",
    "\n",
    "- **open:** prezzo di apertura\n",
    "\n",
    "- **close:** prezzo di chiusura\n",
    "\n",
    "- **adj_close:** prezzo di chiusura modificato\n",
    "\n",
    "- **low:** prezzo minimo\n",
    "\n",
    "- **high:** prezzo massimo\n",
    "\n",
    "- **volume:** numero di transazioni\n",
    "\n",
    "- **date:** data nel formato aaaa-mm-gg\n",
    "\n",
    "il file non presenta campi con valore nullo\n",
    "\n",
    "il minimo valore nel campo 'date' è: '1970-01-02'\n",
    "\n",
    "il massimo valore nel campo 'date' è: '2018-08-24'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>adj_close</th>\n",
       "      <th>low</th>\n",
       "      <th>high</th>\n",
       "      <th>volume</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AHH</td>\n",
       "      <td>11.50</td>\n",
       "      <td>11.58</td>\n",
       "      <td>8.493155</td>\n",
       "      <td>11.25</td>\n",
       "      <td>11.68</td>\n",
       "      <td>4633900</td>\n",
       "      <td>2013-05-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AHH</td>\n",
       "      <td>11.66</td>\n",
       "      <td>11.55</td>\n",
       "      <td>8.471151</td>\n",
       "      <td>11.50</td>\n",
       "      <td>11.66</td>\n",
       "      <td>275800</td>\n",
       "      <td>2013-05-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AHH</td>\n",
       "      <td>11.55</td>\n",
       "      <td>11.60</td>\n",
       "      <td>8.507822</td>\n",
       "      <td>11.50</td>\n",
       "      <td>11.60</td>\n",
       "      <td>277100</td>\n",
       "      <td>2013-05-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AHH</td>\n",
       "      <td>11.63</td>\n",
       "      <td>11.65</td>\n",
       "      <td>8.544494</td>\n",
       "      <td>11.55</td>\n",
       "      <td>11.65</td>\n",
       "      <td>147400</td>\n",
       "      <td>2013-05-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AHH</td>\n",
       "      <td>11.60</td>\n",
       "      <td>11.53</td>\n",
       "      <td>8.456484</td>\n",
       "      <td>11.50</td>\n",
       "      <td>11.60</td>\n",
       "      <td>184100</td>\n",
       "      <td>2013-05-14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker   open  close  adj_close    low   high   volume        date\n",
       "0    AHH  11.50  11.58   8.493155  11.25  11.68  4633900  2013-05-08\n",
       "1    AHH  11.66  11.55   8.471151  11.50  11.66   275800  2013-05-09\n",
       "2    AHH  11.55  11.60   8.507822  11.50  11.60   277100  2013-05-10\n",
       "3    AHH  11.63  11.65   8.544494  11.55  11.65   147400  2013-05-13\n",
       "4    AHH  11.60  11.53   8.456484  11.50  11.60   184100  2013-05-14"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "historical_stock_prices.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**historical_stocks.csv** è composta da 6460 righe e 5 colonne con i seguenti campi:\n",
    "\n",
    "- **ticker:** simbolo univoco dell’azione\n",
    "\n",
    "- **exchange:** NYSE o NASDAQ\n",
    "\n",
    "- **name:** nome dell'azienda\n",
    "\n",
    "- **sector:** settore dell'azienda\n",
    "\n",
    "- **industry:** industria di riferimento per l'azienda\n",
    "\n",
    "il file presenta i campi 'sector' e 'industry' con valori nulli\n",
    "\n",
    "ad un'azienda (campo 'name') possono essere associati anche diversi ticker\n",
    "\n",
    "un'azienda può essere associata a diversi settori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>exchange</th>\n",
       "      <th>name</th>\n",
       "      <th>sector</th>\n",
       "      <th>industry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PIH</td>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>1347 PROPERTY INSURANCE HOLDINGS, INC.</td>\n",
       "      <td>FINANCE</td>\n",
       "      <td>PROPERTY-CASUALTY INSURERS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PIHPP</td>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>1347 PROPERTY INSURANCE HOLDINGS, INC.</td>\n",
       "      <td>FINANCE</td>\n",
       "      <td>PROPERTY-CASUALTY INSURERS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TURN</td>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>180 DEGREE CAPITAL CORP.</td>\n",
       "      <td>FINANCE</td>\n",
       "      <td>FINANCE/INVESTORS SERVICES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FLWS</td>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>1-800 FLOWERS.COM, INC.</td>\n",
       "      <td>CONSUMER SERVICES</td>\n",
       "      <td>OTHER SPECIALTY STORES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FCCY</td>\n",
       "      <td>NASDAQ</td>\n",
       "      <td>1ST CONSTITUTION BANCORP (NJ)</td>\n",
       "      <td>FINANCE</td>\n",
       "      <td>SAVINGS INSTITUTIONS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker exchange                                    name             sector  \\\n",
       "0    PIH   NASDAQ  1347 PROPERTY INSURANCE HOLDINGS, INC.            FINANCE   \n",
       "1  PIHPP   NASDAQ  1347 PROPERTY INSURANCE HOLDINGS, INC.            FINANCE   \n",
       "2   TURN   NASDAQ                180 DEGREE CAPITAL CORP.            FINANCE   \n",
       "3   FLWS   NASDAQ                 1-800 FLOWERS.COM, INC.  CONSUMER SERVICES   \n",
       "4   FCCY   NASDAQ           1ST CONSTITUTION BANCORP (NJ)            FINANCE   \n",
       "\n",
       "                     industry  \n",
       "0  PROPERTY-CASUALTY INSURERS  \n",
       "1  PROPERTY-CASUALTY INSURERS  \n",
       "2  FINANCE/INVESTORS SERVICES  \n",
       "3      OTHER SPECIALTY STORES  \n",
       "4        SAVINGS INSTITUTIONS  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "historical_stocks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Realizzazione Job1\n",
    "### 3.1 Specifiche\n",
    "Realizzare un job che sia in grado di generare le statistiche di ciascuna azione tra il 2008 e il 2018 indicando, per ogni azione: il simbolo, la variazione della quotazione (differenza percentuale arrotondata tra i prezzi di chiusura iniziale e finale dell’intervallo temporale), il prezzo minimo, quello massimo e il volume medio nell’intervallo, ordinando l’elenco in ordine decrescente di variazione della quotazione"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 MapReduce\n",
    "Linguaggio di programmazione utilizzato: **Python** 3.7\n",
    "\n",
    "Per la realizzazione del Job1 in MapReduce è stato sufficiente 1 stage, sono quindi stati implementati 1 mapper e 1 reducer\n",
    "\n",
    "#### Mapper\n",
    "Per realizzare il Job1 è necessario utilizzare solo il file **historical_stock_prices.csv** dunque il mapper svolge solo un'operazione di filtraggio dei dati in base alla data del ticker, accettando solo quelle di interesse (comprese tra il 2008 e il 2018) e passando al reducer solo le colonne 'ticker,close,volume,date' di cui ticker sarà la chiave per lo shuffle and sort; in questa maniera vengono scartate tutte le righe e colonne di dati non necessarie ai fini dell'obiettivo del job riducendo sin da subito anche il traffico di dati all'interno del file system distribuito\n",
    "\n",
    "##### Mapper: pseudocodifica\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![job1_mapper](img/job1_mapper.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reducer\n",
    "Il Reducer prende in input i valori filtrati dal Mapper e riordinati dalla fase di shuffle and sort; ogni stream in input ha dunque i seguenti campi: 'ticker,close,volume,date'. Viene utilizzata una struttura dati di supporto per registrare i dati relativi ad ogni ticker in base al suo obiettivo: cumulare il volume per poi poterne ricavare la media complessiva, registrare il prezzo di chiusura con la data più piccola ed il prezzo di chiusura con la data più grande per poterne ricavare la variazione percentuale, registrare il prezzo minimo e il prezzo massimo.\n",
    "\n",
    "##### Reducer: pseudocodifica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![job1_reducer](img/job1_reducer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Hive\n",
    "\n",
    "Per la realizzazione del Job1 in Hive è necessario creare una tabella dove vengono inseriti i dati del file in input **historical_stock_prices.csv**. Successivamente vengono create diverse Viste: \n",
    "\n",
    "nella Vista 'single_fields_statistics' vengono calcolati il volume medio, il prezzo minimo ed il prezzo massimo in modo semplice grazie al raggruppamento in base al ticker, ovviamente filtrando solo le righe per cui la data è compresa tra il 2008 e il 2018;\n",
    "\n",
    "la Vista 'first_last_date' registra la minima e la massima data disponibile per ogni ticker, questa vista è utile per la creazione delle successive;\n",
    "\n",
    "le Viste 'first_close' e 'last_close' registrano il prezzo relativo rispettivamente alla data minima e alla data massima per ogni ticker;\n",
    "\n",
    "la Vista 'ticker_variation' calcola la variazione  percentuale per ogni ticker sfruttando i valori ricavati dal join delle viste 'first_close' e 'last_close';\n",
    "\n",
    "la Vista 'job1_result' effettua un join tra la 'ticker_variation' e la 'single_fields_statistics' in base al ticker ottenendo così tutti i valori utili per l'output che si potranno ottenere con una semplice SELECT *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![job1_hive1](img/job1_hive.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![job2_spark](img/job1_spark.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Risultati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![job1_results](img/job1_results.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Grafico\n",
    "Il grafico seguente mostra il tempo di esecuzione (l'ascissa indica il tempo in secondi) del Job1 differenziato per tecnologia utilizzata (mapReduce, Hive o Spark), grandezza del dataset in input (25%, 50% o 100%)e luogo fisico di esecuzione (macchina locale o Cluster AWS)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![job1_graphic](img/job1_graphic.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Realizzazione Job2\n",
    "### 4.1 Specifiche\n",
    "Realizzare un job che sia in grado di generare, per ciascun settore, il relativo “trend” nel periodo 2008-2018 ovvero un elenco contenete, per ciascun anno nell’intervallo: il volume annuale medio delle azioni del settore, la variazione annuale media delle aziende del settore e la quotazione giornaliera media delle aziende del settore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 MapReduce\n",
    "Linguaggio di programmazione utilizzato: **Python** 3.7\n",
    "\n",
    "Per la realizzazione del Job2 in MapReduce sono serviti 2 stage, sono quindi stati implementati 2 mapper e 2 reducer. Il primo stage necessario per il join dei due file di input, il secondo per l'elaborazione dell'output.\n",
    "\n",
    "### - Stage 1\n",
    "#### Mapper 1\n",
    "Per realizzare il Job2 è necessario utilizzare sia il file **historical_stock_prices.csv** sia il file **historical_stocks.csv**; dunque il mapper dello stage 1, oltre a svolgere un'operazione di filtraggio dei dati in base alla data del ticker, svolge un'operazione di riconoscimento dell'origine dell'input, passaggio fondamentale per la cooperazione con il reducer che potrà trattare le righe di input in maniera adeguata per ricavare i dati da elaborare. Un modo per riconoscere a quale file di input appartiene lo stream in esame è controllare il numero dei campi, visto che i file di input hanno un numero di colonne diverso tra loro. L'output generato dal mapper sarà composto dal formato 'ticker,close,volume,date,sector' di cui i primi due campi compongono la chiave, strategia utilizzata per ordinare in base al ticker e passare come primo dato utile il settore del ticker.\n",
    "\n",
    "##### Mapper 1: pseudocodifica\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![job2_mapper1](img/job2_mapper1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reducer 1\n",
    "Il Reducer dello stage 1 si occupa del effettivo join dei dati. Sfruttando la fase di shuffle and sort su due chiavi, il reducer riconosce il settore del ticker con il primo stream e lo associa a tutte le righe di dati del ticker\n",
    "\n",
    "##### Reducer 1: pseudocodifica\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![job2_reducer1](img/job2_reducer1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Stage 2\n",
    "#### Mapper 2\n",
    "La fase di mapping dello stage 2 prevede una semplicissima lettura dei dati di input senza operare filtraggi mandando in output stream in formato 'sector,ticker,close,volume,date', in modo da operare il successivo shuffle and sort sul campo 'sector' \n",
    "\n",
    "##### Mapper 2: pseudocodifica\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![job2_mapper2](img/job2_mapper2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reducer 2\n",
    "Il Reducer dello stage 2 utilizza diverse strutture dati di supporto utili per le 2 principali fasi: calcolo dei valori medi annuali('volume', 'close' e variazione percentuale) per ogni ticker, calcolo della media annuale dei valori medi annuali di tutti i ticker di un settore. \n",
    "\n",
    "La prima dictionary ha come chiave la tupla (ticker,year) e come valori vengono cumulati il volume e il close per calcolarne la media, il close con data minima per quel anno e il close con data massima per quel anno.\n",
    "\n",
    "La seconda dictionary ha come chiave la tupla (sector,year) e come valori vengono comulati il volume medio, il close medio e la variation media dei ticker associati al settore per quel anno.\n",
    "\n",
    "##### Reducer 2: pseudocodifica\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![job2_reducer2](img/job2_reducer2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Hive\n",
    "\n",
    "Per la realizzazione del Job2 in Hive è necessario creare due tabelle dove vengono inseriti i dati dei file in input **historical_stock_prices.csv** e **historical_stocks.csv**. Successivamente vengono create diverse Viste: \n",
    "\n",
    "nella Vista 'joined_dataset' viene effettuato il join tra le due tabelle appena popolate filtrando solo le righe per cui la data è compresa tra il 2008 e il 2018 e mantenendo solo le colonne di interesse: 'ticker,volume,close,ticker_year,sector'\n",
    "\n",
    "la Vista 'avg_volume_close' utilizza la vista 'joined_dataset' per calcolare il volume e il close medio raggruppando per sector e year\n",
    "\n",
    "le Vista 'min_date_year' e 'max_date_year' registrano la minima e la massima data disponibile per ogni year per ogni ticker, queste viste sono utili per la creazione delle successive viste 'min_close_year' e 'max_close_year' per ricavare i rispettivi close per poi ricavare la variazione annuale dei ticker nella view 'ticker_variation';\n",
    "\n",
    "le Viste 'min_close_year' e 'max_close_year' registrano il prezzo relativo rispettivamente alla data minima e alla data massima per ogni ticker;\n",
    "\n",
    "la Vista 'sector_variation' calcola la variazione percentuale media delle variazioni percentuali dei ticker per year, associandola al relativo sector;\n",
    "\n",
    "la Vista 'job2_result' effettua un join tra la 'sector_variation' e la 'avg_volume_close' in base al sector ottenendo così tutti i valori utili per l'output che si potranno ottenere con una semplice SELECT *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![job2_hive](img/job2_hive.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![job2_spark](img/job2_spark.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Risultati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![job2_results](img/job2_results.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Grafici\n",
    "Il grafico seguente mostra il tempo di esecuzione (l'ascissa indica il tempo in secondi) del Job2 differenziato per tecnologia utilizzata (mapReduce, Hive o Spark), grandezza del dataset in input (25%, 50% o 100%)e luogo fisico di esecuzione (macchina locale o Cluster AWS)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![job2_graphic](img/job2_graphic.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Realizzazione Job3\n",
    "### 5.1 Specifiche\n",
    "Realizzare un job in grado di generare gruppi di aziende le cui azioni hanno avuto lo stesso trend in termini di variazione annualenell’ultimo triennio disponibile, indicando le aziende e il trend comune (es. {Apple, Intel, Amazon}: 2016:-1%, 2017:+3%,2018:+5%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 MapReduce\n",
    "Linguaggio di programmazione utilizzato: **Python** 3.7\n",
    "\n",
    "Per la realizzazione del Job3 in MapReduce sono serviti 2 stage, sono quindi stati implementati 2 mapper e 2 reducer. Il primo stage necessario per il join dei due file di input, il secondo per l'elaborazione dell'output.\n",
    "\n",
    "### - Stage 1\n",
    "#### Mapper 1\n",
    "Per realizzare il Job3 è necessario anche qui utilizzare sia il file **historical_stock_prices.csv** sia il file **historical_stocks.csv**; dunque il mapper dello stage 1, oltre a svolgere un'operazione di filtraggio dei dati in base alla data del ticker, svolge un'operazione di riconoscimento dell'origine dell'input. L'output generato dal mapper sarà composto dal formato 'ticker,close,date,name'.\n",
    "\n",
    "##### Mapper 1: pseudocodifica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![job3_mapper1](img/job3_mapper1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reducer 1\n",
    "Il Reducer dello stage 1 si occupa del effettivo join dei dati. Sfruttando una struttura dati con chiave (ticker,year) registra i 'close' con data minima e data massima per un certo ticker in un certo anno, in questo modo si riesce a calcolare la variazione percentuale annuale dei ticker sin dal primo reducer in modo da diminuire il flusso di dati toale. L'output generato sarà così composto 'name,ticker,year,variation' quindi come chiave avremo il nome dell'azienda\n",
    "\n",
    "##### Reducer 1: pseudocodifica\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![job3_reducer1](img/job3_reducer1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Stage 2\n",
    "#### Mapper 2\n",
    "La fase di mapping dello stage 2 prevede una semplicissima lettura dei dati di input senza operare filtraggi mandando in output stream in formato 'name,ticker,close,volume,date', in modo da operare il successivo shuffle and sort sul campo 'name' riferito al nome dell'azienda\n",
    "\n",
    "##### Mapper 2: pseudocodifica\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![job3_mapper2](img/job3_mapper2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reducer 2\n",
    "Il Reducer dello stage 3 utilizza diverse strutture dati di supporto utili per le 3 principali fasi: calcolo della variazione percentuale media annua per ogni azienda, confronto con le variazioni percentuali annue delle altre aziende e raggruppamento di aziende con trend uguali\n",
    "\n",
    "La prima dictionary ha come chiave la tupla (company,year) e come valori vengono cumulate le variazioni percentuali dei ticker associati all'azienda per calcolarne la media.\n",
    "\n",
    "La seconda dictionary ha come chiave 'company' e come valore una lista delle variazioni percentuali medie calcolate e posizionate in ordine annuo in modo tale da poter confrontare direttamente le liste nella fase successiva per poter trovare le aziende con trend uguali.\n",
    "\n",
    "La terza struttura dati è una lista di liste, dove il campo 1 viene popolato con una lista di aziende con trend uguali e il campo 2 contiene il relativo trend in ordine annuo\n",
    "\n",
    "##### Reducer 2: pseudocodifica\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![job3_reducer2](img/job3_reducer2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Hive\n",
    "\n",
    "Per la realizzazione del Job3 in Hive è necessario creare due tabelle dove vengono inseriti i dati dei file in input **historical_stock_prices.csv** e **historical_stocks.csv**. Successivamente vengono create diverse Viste: \n",
    "\n",
    "nella Vista 'joined_dataset' viene effettuato il join tra le due tabelle appena popolate filtrando solo le righe per cui la data è compresa tra il 2016 e il 2018 e mantenendo solo le colonne di interesse: 'ticker,volume,close,ticker_year,name'\n",
    "\n",
    "le Vista 'min_date_year' e 'max_date_year' registrano la minima e la massima data disponibile per ogni year per ogni ticker, queste viste sono utili per la creazione delle successive viste 'min_close_year' e 'max_close_year' per ricavare i rispettivi close per poi ricavare la variazione annuale dei ticker nella view 'ticker_variation';\n",
    "\n",
    "le Viste 'min_close_year' e 'max_close_year' registrano il prezzo relativo rispettivamente alla data minima e alla data massima per ogni ticker;\n",
    "\n",
    "la Vista 'ticker_variation' calcola la variazione percentuale di ogni ticker per ogni anno;\n",
    "\n",
    "la Vista 'company_variation' calcola la variazione percentuale media delle variazioni percentuali dei ticker per year, associandola alla relativa azienda;\n",
    "\n",
    "la Vista 'company_trend' collezione le variazioni di ogni azienda in una lista in modo da avere i campi 'company' e 'trend', formato molto utile per la vista successiva;\n",
    "\n",
    "la Vista 'company_trend' colleziona le variazioni di ogni azienda in una lista in modo da avere i campi 'company' e 'trend', formato molto utile per la vista successiva;\n",
    "\n",
    "la Vista 'job3_result' sfrutta il formato della vista 'company_trend' per la ricerca delle aziende con trend uguali, basta infatti raggruppare per il campo 'trend' e collezionare le aziende associate in una lista."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![job3_hive1](img/job3_hive1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![job3_spark](img/job3_spark.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Risultati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![job3_results](img/job3_results.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Grafici\n",
    "Il grafico seguente mostra il tempo di esecuzione (l'ascissa indica il tempo in secondi) del Job3 differenziato per tecnologia utilizzata (mapReduce, Hive o Spark), grandezza del dataset in input (25%, 50% o 100%)e luogo fisico di esecuzione (macchina locale o Cluster AWS)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![job3_graphic](img/job3_graphic.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusioni\n",
    "\n",
    "La tabella seguente mostra i tempi di esecuzione (in secondi) di tutti i Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![timings](img/timings.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Osservando tutti i grafici e la tabella finale si evince che:\n",
    "\n",
    "- Tutti i job eseguiti in Hive hanno un tempo di esecuzione molto più alto rispetto ai medesimi eseguiti in MapReduce o Spark.\n",
    "\n",
    "- In MapReduce, nella fase di Mapping la scelta delle chiavi su cui verranno eseguite le operazioni di shuffle and sort è fondamentale in termini di tempo\n",
    "\n",
    "- I vantaggi delle esecuzioni dei job su Cluster aumentano col crescere del dataset in input, per tutte le tecnologie utilizzate.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
